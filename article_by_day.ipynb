{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_commonhealth_url = \"https://www.commonhealth.com.tw/channel/48?page={}&tab=all\"\n",
    "\n",
    "df = pd.DataFrame(columns=[\"date\",\"title\",\"url\",\"watch\",\"content\"])\n",
    "\n",
    "for page in range(1,34):\n",
    "    print(page)\n",
    "    fc_res = requests.get(food_commonhealth_url.format(page), headers=headers)\n",
    "    fc_soup = BeautifulSoup(fc_res.text, \"html.parser\")\n",
    "    fc_new_soup = fc_soup.select(\"div[class='tab__target']\")[0]\n",
    "    fc_tmp_titles = fc_new_soup.select(\"div[class='title']\")\n",
    "    fc_urls = fc_soup.select(\"a[class='recommend recommend--channels']\")\n",
    "\n",
    "    for i,j in zip(range(1,10),range(0,9)):   #每頁扣除第一篇還有九篇\n",
    "\n",
    "        fc_title = fc_tmp_titles[i].text\n",
    "        fc_article_url = fc_urls[j][\"href\"]\n",
    "        print(fc_title)\n",
    "        print(fc_article_url)\n",
    "        fc_article_res = requests.get(fc_article_url,headers=headers)\n",
    "        fc_article_soup = BeautifulSoup(fc_article_res.text, \"html.parser\")        \n",
    "        fc_article = fc_article_soup.select(\"div[class='essay']\")\n",
    "        if len(fc_article_soup.select(\"div[class='info__line info__line--view']\")) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            tmp_watch = fc_article_soup.select(\"div[class='info__line info__line--view']\")[0].text\n",
    "            fc_watch = re.sub(u\"([^\\u0030-\\u0039])\",\"\",tmp_watch)\n",
    "            print(fc_watch)\n",
    "            tmp_date = fc_article_soup.select(\"span[id='publish_time']\")[0].text\n",
    "            date = datetime.strptime(tmp_date, \"%Y/%m/%d\")\n",
    "            print(date)\n",
    "            if len(fc_article) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                print(fc_article[0].text)\n",
    "\n",
    "                row = pd.DataFrame([[date,fc_title,fc_article_url,fc_watch,fc_article[0].text]],columns=[\"date\",\"title\",\"url\",\"watch\",\"content\"])\n",
    "                df = df.append(row, ignore_index = True)\n",
    "    if page % 3 == 0:\n",
    "        time.sleep(12)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edh_url = \"https://www.edh.tw/category/218/index/{}\"\n",
    "\n",
    "for page in range(1,20): \n",
    "    edh_res = requests.get(edh_url.format(page),headers)\n",
    "    edh_soup = BeautifulSoup(edh_res.text, \"html.parser\")\n",
    "    edh_tmp_soup = edh_soup.select(\"div[class='article-grid clear']\")\n",
    "    for i in range(2):     # 文章分為兩部分，分別爬取\n",
    "        \n",
    "        edh_new_soup = edh_tmp_soup[i]\n",
    "        if len(edh_new_soup) == 0:    #如無內容則 PASS\n",
    "            pass\n",
    "        for j in edh_new_soup.select(\"a[class='detail']\"):\n",
    "            edh_title = j.select(\"h3[class='title']\")[0].text\n",
    "            edh_article_url = \"https://www.edh.tw/\" + j[\"href\"] \n",
    "            print(edh_title)\n",
    "            print(edh_article_url)\n",
    "            watch_res = requests.get(edh_article_url, headers)\n",
    "            watch_soup = BeautifulSoup(watch_res.text, \"html.parser\")\n",
    "            tmp_watch = watch_soup.select(\"span[class='number']\")[0].text\n",
    "            edh_watch = re.sub(u\"([^\\u0030-\\u0039])\",\"\",tmp_watch)\n",
    "            print(edh_watch)\n",
    "            \n",
    "            tmp_date = watch_soup.select(\"span[class='date']\")[0].text\n",
    "            date = datetime.strptime(tmp_date, \"%Y-%m-%d\")\n",
    "\n",
    "            edh_article = \" \"\n",
    "            for k in range(1,3):\n",
    "                edh_tmp_article_url = edh_article_url + \"/{}\"\n",
    "                edh_article_res = requests.get(edh_tmp_article_url.format(k),headers)\n",
    "                edh_article_soup = BeautifulSoup(edh_article_res.text, \"html.parser\")\n",
    "                edh_tmp_article = edh_article_soup.select(\"div[id='article_page']\")\n",
    "\n",
    "                if len(edh_tmp_article) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    edh_article += edh_tmp_article[0].text\n",
    "            print(edh_article)\n",
    "            \n",
    "            row = pd.DataFrame([[date,edh_title,edh_article_url,edh_watch,edh_article]],columns=[\"date\",\"title\",\"url\",\"watch\",\"content\"])\n",
    "            df = df.append(row, ignore_index = True)\n",
    "            \n",
    "    if page % 3 == 0:\n",
    "        time.sleep(12)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by = [\"date\"], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"./article_by_date.csv\", encoding = \"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_content = df.drop(columns=[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_no_content.to_csv(\"./article_no_content.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./article_no_content.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
